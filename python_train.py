# -*- coding: utf-8 -*-
"""python train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TzP7D_RDXdkDD0lbQMw97lHwMWsWRdtj
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
import numpy as np
from sklearn.metrics import roc_auc_score, f1_score, recall_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from PIL import Image

# --- Hyperparameters ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 64
LEARNING_RATE = 1e-4
NUM_EPOCHS = 10
WEIGHT_DECAY = 1e-4

# --- Custom Dataset Class ---
class PneumoniaMNISTNPZ(Dataset):
    def __init__(self, npz_path, split='train', transform=None):
        self.data = np.load(npz_path)
        self.transform = transform

        if split == 'train':
            self.images = self.data['train_images']
            self.labels = self.data['train_labels']
        elif split == 'val':
            self.images = self.data['val_images']
            self.labels = self.data['val_labels']
        elif split == 'test':
            self.images = self.data['test_images']
            self.labels = self.data['test_labels']
        else:
            raise ValueError("split must be one of 'train', 'val', or 'test'")

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = int(self.labels[idx])

        # Convert (28,28) to PIL Image and apply transforms
        image = Image.fromarray(image.astype(np.uint8))
        if self.transform:
            image = self.transform(image)

        return image, label

def main():
    print(f"Using device: {DEVICE}")

    # --- Transforms ---
    transform_train = transforms.Compose([
        transforms.Resize((299, 299)),
        transforms.RandomRotation(10),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Grayscale to 3 channels
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    transform_test = transforms.Compose([
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    # --- Load Data ---
    npz_path = '/content/drive/MyDrive/pneumoniamnist.npz'
    train_dataset = PneumoniaMNISTNPZ(npz_path, split='train', transform=transform_train)
    val_dataset = PneumoniaMNISTNPZ(npz_path, split='val', transform=transform_test)
    test_dataset = PneumoniaMNISTNPZ(npz_path, split='test', transform=transform_test)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --- Class Weights ---
    labels = np.array([label for _, label in train_dataset])
    num_normal = np.sum(labels == 0)
    num_pneumonia = np.sum(labels == 1)
    print(f"Training set class distribution: Normal={num_normal}, Pneumonia={num_pneumonia}")

    total_samples = len(train_dataset)
    class_weights = torch.tensor([
        total_samples / (2 * num_normal),
        total_samples / (2 * num_pneumonia)
    ], dtype=torch.float).to(DEVICE)
    print(f"Calculated class weights: {class_weights}")

    # --- Model ---
    model = models.inception_v3(weights='Inception_V3_Weights.DEFAULT')
    for param in model.parameters():
        param.requires_grad = False

    if model.AuxLogits is not None:
        num_ftrs_aux = model.AuxLogits.fc.in_features
        model.AuxLogits.fc = nn.Linear(num_ftrs_aux, 2)

    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Linear(num_ftrs, 512),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(512, 2)
    )

    for param in model.fc.parameters():
        param.requires_grad = True
    if model.AuxLogits is not None:
        for param in model.AuxLogits.fc.parameters():
            param.requires_grad = True

    model = model.to(DEVICE)

    # --- Loss & Optimizer ---
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    # --- Training Loop ---
    print("\nStarting training...")
    for epoch in range(NUM_EPOCHS):
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images = images.to(DEVICE)
            labels = labels.to(DEVICE)

            optimizer.zero_grad()
            outputs, aux_outputs = model(images)
            loss = criterion(outputs, labels) + 0.4 * criterion(aux_outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(DEVICE)
                labels = labels.to(DEVICE)
                outputs = model(images)
                if isinstance(outputs, tuple): outputs = outputs[0]
                loss = criterion(outputs, labels)
                val_loss += loss.item()

        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}")

    # --- Evaluation ---
    print("\nEvaluating on the test set...")
    model.eval()
    all_labels = []
    all_preds = []
    all_probs = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(DEVICE)
            labels = labels.to(DEVICE)
            outputs = model(images)
            if isinstance(outputs, tuple): outputs = outputs[0]
            probs = torch.nn.functional.softmax(outputs, dim=1)[:, 1]
            _, preds = torch.max(outputs, 1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    auc = roc_auc_score(all_labels, all_probs)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)

    print("\n--- Final Performance Metrics ---")
    print(f"AUC-ROC: {auc:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"Recall: {recall:.4f}")

    # --- Confusion Matrix ---
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Pneumonia'],
                yticklabels=['Normal', 'Pneumonia'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from sklearn.metrics import roc_auc_score, f1_score, recall_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from PIL import Image
import copy

# --- Hyperparameters ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 32
INITIAL_LR = 3e-4
FINETUNE_LR = 1e-5
NUM_EPOCHS_HEAD = 5
NUM_EPOCHS_FINETUNE = 10
WEIGHT_DECAY = 1e-4
PATIENCE = 3

# --- Custom Dataset Class ---
class PneumoniaMNISTNPZ(Dataset):
    def __init__(self, npz_path, split='train', transform=None):
        self.data = np.load(npz_path)
        self.transform = transform
        if split == 'train':
            self.images, self.labels = self.data['train_images'], self.data['train_labels']
        elif split == 'val':
            self.images, self.labels = self.data['val_images'], self.data['val_labels']
        elif split == 'test':
            self.images, self.labels = self.data['test_images'], self.data['test_labels']
        else:
            raise ValueError("split must be one of 'train', 'val', or 'test'")

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image, label = self.images[idx], int(self.labels[idx])
        image = Image.fromarray(image.astype(np.uint8))
        if self.transform:
            image = self.transform(image)
        return image, label

def print_strategy_explanations():
    """Prints the detailed explanations for the assignment questions."""
    print("=================================================================")
    print("            Project Strategy and Methodology")
    print("=================================================================\n")
    print("--- a. Choice of Evaluation Metrics ---")
    print("1. AUC-ROC: Measures the model's ability to distinguish between classes across all thresholds, crucial for imbalanced medical data.")
    print("2. F1-Score: A balanced measure of precision and recall, providing a robust performance indicator.")
    print("3. Recall (Sensitivity): Critical for minimizing false negatives (missed pneumonia cases).\n")
    print("--- b. Detecting and Mitigating Class Imbalance ---")
    print("1. Detection: By counting class instances in the training set.")
    print("2. Mitigation: Using a weighted Cross-Entropy Loss to give higher penalty for misclassifying the minority class.\n")
    print("--- c. Measures to Prevent Over-fitting ---")
    print("1. Data Augmentation: Applying random flips, rotations, and shifts to training images.")
    print("2. Dropout: Added to the classifier head to prevent co-adaptation of neurons.")
    print("3. Weight Decay (L2 Regularization): Used in the AdamW optimizer.")
    print("4. Early Stopping & LR Scheduler: To halt training at the optimal point and adjust learning rate dynamically.\n")
    print("=================================================================\n")

def main():
    print_strategy_explanations()
    print(f"Using device: {DEVICE}")

    transform_train = transforms.Compose([
        transforms.Resize((299, 299)), transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15), transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=10),
        transforms.ToTensor(), transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    transform_test = transforms.Compose([
        transforms.Resize((299, 299)), transforms.ToTensor(),
        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    npz_path = '/content/drive/MyDrive/pneumoniamnist.npz'
    train_dataset = PneumoniaMNISTNPZ(npz_path, split='train', transform=transform_train)
    val_dataset = PneumoniaMNISTNPZ(npz_path, split='val', transform=transform_test)
    test_dataset = PneumoniaMNISTNPZ(npz_path, split='test', transform=transform_test)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

    labels = np.array([label for _, label in train_dataset])
    num_normal, num_pneumonia = np.sum(labels == 0), np.sum(labels == 1)
    print(f"Detected Training Set Distribution: Normal={num_normal}, Pneumonia={num_pneumonia}")
    total_samples = len(train_dataset)
    class_weights = torch.tensor([total_samples / (2 * num_normal), total_samples / (2 * num_pneumonia)], dtype=torch.float).to(DEVICE)
    print(f"Calculated class weights for mitigation: {class_weights}\n")

    model = models.inception_v3(weights='Inception_V3_Weights.DEFAULT')
    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Linear(num_ftrs, 512), nn.BatchNorm1d(512), nn.ReLU(),
        nn.Dropout(0.5), nn.Linear(512, 2)
    )
    if model.AuxLogits is not None:
        model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, 2)
    model = model.to(DEVICE)

    criterion = nn.CrossEntropyLoss(weight=class_weights)
    best_model_wts, best_val_loss, epochs_no_improve = copy.deepcopy(model.state_dict()), float('inf'), 0

    print("--- Phase 1: Training the Classifier Head ---")
    for param in model.parameters(): param.requires_grad = False
    for param in model.fc.parameters(): param.requires_grad = True
    if model.AuxLogits is not None:
        for param in model.AuxLogits.fc.parameters(): param.requires_grad = True
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY)
    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=PATIENCE-1, verbose=True)
    for epoch in range(NUM_EPOCHS_HEAD):
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs, aux_outputs = model(images)
            loss = criterion(outputs, labels) + 0.4 * criterion(aux_outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)
        val_loss = 0.0
        model.eval()
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * images.size(0)
        epoch_loss, val_loss = running_loss / len(train_dataset), val_loss / len(val_dataset)
        print(f"Epoch {epoch+1}/{NUM_EPOCHS_HEAD} -> Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
        scheduler.step(val_loss)

    print("\n--- Phase 2: Fine-tuning all Layers ---")
    for param in model.parameters(): param.requires_grad = True
    optimizer = optim.AdamW(model.parameters(), lr=FINETUNE_LR, weight_decay=WEIGHT_DECAY)
    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=PATIENCE-1, verbose=True)
    for epoch in range(NUM_EPOCHS_FINETUNE):
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()

            # ############################################################### #
            # ## --- FIX IS HERE --- ##
            # Unpack the main and auxiliary outputs, just like in Phase 1
            outputs, aux_outputs = model(images)
            loss1 = criterion(outputs, labels)
            loss2 = criterion(aux_outputs, labels)
            # Calculate the combined loss
            loss = loss1 + 0.4 * loss2
            # ############################################################### #

            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)

        val_loss = 0.0
        model.eval()
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * images.size(0)
        epoch_loss, val_loss = running_loss / len(train_dataset), val_loss / len(val_dataset)
        print(f"Epoch {epoch+1}/{NUM_EPOCHS_FINETUNE} -> Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
        scheduler.step(val_loss)
        if val_loss < best_val_loss:
            best_val_loss, best_model_wts, epochs_no_improve = val_loss, copy.deepcopy(model.state_dict()), 0
            print("Validation loss decreased. Saving model...")
        else:
            epochs_no_improve += 1
        if epochs_no_improve >= PATIENCE:
            print(f"Early stopping triggered after {epoch+1} epochs.")
            break

    print("\nLoading best model for evaluation...")
    model.load_state_dict(best_model_wts)
    print("Evaluating on the test set...")
    model.eval()
    all_labels, all_preds, all_probs = [], [], []
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(DEVICE)
            outputs = model(images)
            probs, preds = torch.nn.functional.softmax(outputs, dim=1)[:, 1], torch.max(outputs, 1)[1]
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    auc, f1, recall = roc_auc_score(all_labels, all_probs), f1_score(all_labels, all_preds), recall_score(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)
    print("\n--- Final Performance Metrics ---")
    print(f"AUC-ROC: {auc:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"Recall (Sensitivity): {recall:.4f}")
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix on Test Set')
    plt.show()

if __name__ == '__main__':
    main()